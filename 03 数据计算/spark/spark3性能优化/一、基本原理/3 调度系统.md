# 3 调度系统

Spark 是典型的主从型（M/S，Master/Slave）架构，从系统的角度来看，Spark 分布式系统的核心进程只有两种：Driver 和 Executor，分别对应主从架构的 Master 和 Slave。

在 Spark 分布式系统中，Driver 只有一个，而 Executor 可以有多个。

Driver 提供 SparkContext（SparkSession）上下文环境，而上下文环境提供了 Spark 分布式系统所有的核心组件，如 RPC 系统、调度系统、存储系统、内存管理、Shuffle 管理等。

Driver 通过这些核心组件来对分布式任务进行拆解、调度、分发、追踪与维护，俗称“指挥的”。

Executor 主要负责分布式任务的执行和状态反馈，俗称“干活儿的”。

**Spark 调度系统的核心职责是**：先将用户构建的 DAG 转化为分布式任务，结合分布式集群资源的可用性，基于调度规则依序把分布式任务分发到执行器。

**Spark 调度系统的工作流程包含如下 5 个步骤：**

    1. 将 DAG 拆分为不同的运行阶段 Stages；
    
    2. 创建分布式任务 Tasks 和任务组 TaskSet；
    
    3. 获取集群内可用的硬件资源情况；
    
    4. 按照调度规则决定优先调度哪些任务/组；
    
    5. 依序将分布式任务分发到执行器 Executor。

**调度系统中的核心组件有哪些？**

Spark 调度系统包含 3 个核心组件，分别是 DAGScheduler、TaskScheduler 和 SchedulerBackend。这 3 个组件都运行在 Driver 进程中，它们通力合作将用户构建的 DAG 转化为分布式任务，再把这些任务分发给集群中的 Executors 去执行。

|  调度流程步骤   | 调度系统组件  |
|  :----:  | :----:  |
| 将 DAG 拆分为不同的运行阶段 Stages  | DAGScheduler |
| 创建分布式任务 Tasks 和任务组 TaskSet  | DAGScheduler |
| 获取集群内可用的硬件资源情况  | SchedulerBackend |
| 按照调度规则决定优先调度哪些任务/组  | TaskScheduler |
| 依序将分布式任务分发到执行器 Executor  | SchedulerBackend |

```mermaid
sequenceDiagram
title:调度简易流程图
autonumber

participant D as DAGScheduler
participant T as TaskScheduler
participant S as SchedulerBackend
participant E as ExcutorBackend

D->>T: Task/TaskSet
S->>T: WorkOffer
T->>T: 任务调度
T->>S: LaunchTask
S->>E: LaunchTask
E->>E: TaskRunner.run()
E->>S: StatusUpdate
S->>T: StatusUpdate
T->>D: StatusUpdate
```
**[DAGScheduler](https://github.com/apache/spark/blob/v3.2.1/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala)：**

DAGScheduler的主要职责：一是把用户 DAG 拆分为 Stages；二是在 Stage 内创建计算任务 Tasks，这些任务囊括了用户通过组合不同算子实现的数据转换逻辑。

**[SchedulerBackend](https://github.com/apache/spark/blob/v3.2.1/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala)：**

SchedulerBackend主要是判断节点是否空闲，是对于资源调度器的封装与抽象，为了支持多样的资源调度模式如 Standalone、YARN 和 Mesos，SchedulerBackend 提供了对应的实现类。
在运行时，Spark 根据用户提供的 MasterURL，来决定实例化哪种实现类的对象。MasterURL 就是你通过各种方式指定的资源管理器，如 --master spark://ip:host（Standalone 模式）、--master yarn（YARN 模式）。

SchedulerBackend 用一个叫做 ExecutorDataMap 的数据结构，来记录每一个计算节点中 Executors 的资源状态。
ExecutorDataMap 是一种 HashMap，它的 Key 是标记 Executor 的字符串，Value 是一种叫做 ExecutorData 的数据结构，
ExecutorData 用于封装 Executor 的资源状态，如 RPC 地址、主机地址、可用 CPU 核数和满配 CPU 核数等等，它相当于是对 Executor 做的“资源画像”。

    为什么ExecutorData不存储于内存相关的信息。
    答案是：不需要。
    (1)TaskScheduler要达到目的，它只需知道Executors是否有空闲CPU、有几个空闲CPU就可以了，有这些信息就足以让他决定是否把tasks调度到目标Executors上去。
    (2)每个Executors的内存总大小，在Spark集群启动的时候就确定了，因此，ExecutorData自然是没必要记录像Total Memory这样的冗余信息。
    (3)Spark对于内存的预估不准，每个Executors的可用内存都会随着GC的执行而动态变化，ExecutorData记录的Free Memory，永远都是过时的信息，TaskScheduler拿到这样的信息，也没啥用。

对内，SchedulerBackend 用 ExecutorData 对 Executor 进行资源画像；
对外，SchedulerBackend 以 WorkerOffer 为粒度提供计算资源，WorkerOffer 封装了 Executor ID、主机地址和 CPU 核数，用来表示一份可用于调度任务的空闲资源。

**[TaskScheduler](https://github.com/apache/spark/blob/v3.2.1/core/src/main/scala/org/apache/spark/scheduler/TaskScheduler.scala)：**

TaskScheduler 的职责是：基于既定的规则与策略达成 DAGScheduler+SchedulerBackend 双方的匹配与撮合。
TaskScheduler 的调度策略分为两个层次，一个是不同 Stages 之间的调度优先级，一个是 Stages 内不同task之间的调度优先级。TaskScheduler会根据任务的本地性要求，按照“数据不动代码动”的原则进行任务的调度。

